---
title: "Identifying Voting Coalitions in DAOs"
output:
  html_document: default
  pdf_document: default
date: '2022-06-23'
bibliography: refs.bib
csl: ieee.csl
---

This document is has all of the code used to produce the analysis in the Medium article: https://medium.com/metagov/how-to-think-about-voting-systems-2494560c4c82

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(combinat)
library(qgraph)
library(igraph)
library(factoextra)
library(usedist) # to compute dice metric
library(hopkins)
library(seriation)
```

# Data

```{r}
votes <- read.csv("votes.csv", colClasses = c("Voter.ID" = "character")) # Voter.ID will be read as a hex number if type is unspecified
```

```{r clean, include=FALSE}
# restrict our attention to proposals not made before 0x824953e27fca1b0dbc0242b82750afbd2efb6b60621a6383674653bc826ef9c8
start_time <- votes %>%
  filter(Proposal.ID == "0x824953e27fca1b0dbc0242b82750afbd2efb6b60621a6383674653bc826ef9c8") %>%
  arrange(Time.Created) %>%
  head(1)
 
start_time <- start_time$Time.Created
v1 <- votes %>%
  filter(Time.Created >= start_time)
# count number of votes cast per voter
v1 <- v1 %>%
  group_by(Voter.ID) %>%
  mutate(N.Votes = n())
length(unique(v1$Voter.ID)) # we have 658  individual voters after this time
length(unique(v1$Proposal.ID)) # and 7 proposals
# and 69 voters who cast at least 4 votes
v1 %>%
  filter(N.Votes >= 4) %>%
  select(Voter.ID) %>%
  unique() %>%
  nrow()
# lets focus on these repeat voters
v1 <- v1 %>%
  filter(N.Votes >= 4) 
```

This leaves us with a very manageable set of 69 voters and 7 proposals.


# Analysis
## Generating a Voting Graph

To start off, we'll organize our data into a table with individual voters as the rows, proposals as the columns, and cells corresponding to voters' choices.

```{r vot_tab, include=FALSE}
# make a table with rows=voters and cols=proposals, with cell values=how they voted.
proposals <- unique(v1$Proposal.ID)
voters <- unique(v1$Voter.ID)
vot_tab <- data.frame(matrix(nrow = length(voters), ncol= length(proposals)))
colnames(vot_tab) <- proposals
rownames(vot_tab) <- voters
# Note: this code is highly inefficient and could be made much faster.
for(i in 1:nrow(vot_tab)) {
  for(j in 1:ncol(vot_tab)) {
    row <- filter(v1, Voter.ID == voters[i], Proposal.ID == proposals[j])
    if(nrow(row) != 0) {
      vot_tab[i,j] <- row$Choice
    }
  }
}
# make the cols a little nicer
colnames(vot_tab) <- 1:length(proposals)
rownames(vot_tab) <- 1:length(voters)
```

```{r}
# how many missing values do we have?
na_counts <- rowSums(is.na(vot_tab))
table(na_counts)
mean(na_counts)
```

Six voters voted on every proposal, 22 missed at most one, and 37 missed at most two. The average voter in our set missed about two of the seven votes we're looking at. 

```{r}
# OPTION 1 (best?)
# NAs just aren't considered in denominator: (NA, NA, 1), (NA, NA, 1) is 0 distance, (NA, NA, 1), (NA, NA, 2) is 1 distance
dice_metric <- function(x, y) {
  n_votes <- c(length(which(!is.na(x))), length(which(!is.na(y)))) 
  1 - 2*length(which(x == y))/sum(n_votes) # 1 - dice coefficient (to make it a metric)
                                         # Dice coeff: 2*| X intersect Y | / |X| + |Y|
}

# OPTION 2 (worst)
# # NAs are counted to the totals but not vote similarities: (NA, NA, 1), (NA, NA, 1) is 1/3 distance, (NA, NA, 1), (NA, NA, 2) is 1 distance
# 
# dice_metric <- function (x, y) 1 - (2*length(which(x == y))/(length(x) + length(y))) # 1 - dice coefficient (to make it a metric)
#                                                                                      # Dice coeff: 2*| X intersect Y | / |X| + |Y|
# 

# OPTION 3 (not bad)
# # NAs are counted as as a third vote: (NA, NA, 1), (NA, NA, 1) is 0 distance, (NA, NA, 1), (NA, NA, 2) is 2/3 distance
# # we're treating NAs, 1s & 2s all as categorical options. Possible downside is that abstensions are weighted as regular votes.
# vot_tab[is.na(vot_tab)] <- "a"
# vot_tab[vot_tab == 1] <- "b"
# vot_tab[vot_tab == 2] <- "c"
# dice_metric <- function (x, y) 1 - (2*length(which(x == y))/(length(x) + length(y))) # 1 - dice coefficient (to make it a metric)
#                                                                                      # Dice coeff: 2*| X intersect Y | / |X| + |Y|


vot_mat <- data.matrix(vot_tab, rownames.force = NA)

dist <- dist_make(vot_mat, dice_metric)

dist_mat <- as.matrix(dist)
```


Now, we can represent the voting similarities as a weighted graph.

```{r graph, include=FALSE}
# Make a voting graph
jpeg('votegraph.jpg', width=1000, height=1000, unit='px')
qgraph(1 - dist_mat, layout='spring', vsize=3)
dev.off()
```

![](votegraph.jpg)



## Identifying Coalitions

We can see visually from our graph that the right answer seems to be that the data *does* present clusters, and that there are basically two, along with a bunch of relatively independent voters. However, since this post is more about methodology than the specific analysis of this data, we'll show how you might interrogate this question quantitatively if you had a more complex or ambiguous dataset. 

### Hopkins
The usual tool is the [Hopkins statistic](https://en.wikipedia.org/wiki/Hopkins_statistic), defined as:

${\displaystyle H={\frac {\sum _{i=1}^{m}{u_{i}^{d}}}{\sum _{i=1}^{m}{u_{i}^{d}}+\sum _{i=1}^{m}{w_{i}^{d}}}}\,},$


```{r}
hops <- c()
for(i in 1:1000) {
  vot_tab[is.na(vot_tab)] <- 0
  hops <- append(hops,hopkins(vot_tab, ceiling(nrow(vot_tab)/4)))
}
mean(hops)
```


### VAT

```{r}
dissplot(dist_mat)
```


### Hierarchical Clustering

In order to get a rough power distribution, we can define "coalitions" or "voting blocs" by hierarchically clustering on this graph.

```{r}
hc <- hclust(dist, method = "average")
plot(hc, main = "Dendrogram of Voting Preferences", xlab = "", sub = "",
     cex = 0.5)
```


### Investigate optimal number of clusters

**Note:** I'm choosing complete linkage clustering and a number of clusters/threshold for voting agreement within coalitions pretty arbitrarily. One can imagine only defining coalitions which agree completely, or which correspond to prior knowledge of actual actors within a system. Nonetheless, I think it's an interesting exercise.

```{r}
fviz_nbclust(vot_tab, hcut, method = "silhouette", diss = dist, k.max = 24)
fviz_nbclust(vot_tab, hcut, method = "wss", diss = dist, k.max = 24)
fviz_nbclust(vot_tab, hcut, method = "gap_stat", diss = dist, k.max = 24)
```

### Assign Blocs

```{r}
# Cluster the voters into voting "blocs"
# hc_clusters <- cutree(hc, h = 0.88)
hc_clusters <- cutree(hc, 5)
```

```{r blocs, include=FALSE}
v2 <- v1 %>%
  add_column(Bloc = NA)
for(i in 1:length(voters)) {
  id <- voters[i]
  rows <- which(v2$Voter.ID == id)
  v2$Bloc[rows] <- hc_clusters[i]
}
```

```{r bloc1, include=FALSE}
# count each Bloc's voting weight, using each voter's max weight
v2 <- v2 %>%
  group_by(Voter.ID) %>%
  mutate(Max.Weight = max(Weight))

# get a list of weights per voter
weights <- c()
for(i in 1:length(voters)) {
  sample <- v2 %>%
    filter(Voter.ID == voters[i]) %>%
    head(1)
  weights <- append(weights,sample$Max.Weight)
}
```

```{r}
# make a bar chart of voters' weights, colored by Bloc

voter_weights_blocs <- data.frame(Weight = weights, Bloc = hc_clusters)
voter_weights_blocs$ID <- rownames(voter_weights_blocs)
voter_weights_blocs <- within(voter_weights_blocs, Factor <- ave(as.character(Weight),FUN=make.unique)) # make a sortable factor based on Weight

# figure out how to order things by weight
order <- reorder(voter_weights_blocs$Factor, sort(as.numeric(voter_weights_blocs$Factor), decreasing = TRUE))
levels(order) <- sort(as.numeric(levels(order)))
voter_weights_blocs$order <- factor(voter_weights_blocs$Weight, levels = unique(order))
```

```{r}
voter_weights_blocs %>%
  ggplot(aes(x = order, y = Weight/sum(Weight), fill = factor(Bloc))) +
  geom_col(color = "black") +
  # theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  ggtitle("Individual Voters by Weight") +
  xlab("Voter Weight") +
  ylab("Portion of Total Weight") + 
  labs(fill = "Coalition") +
  scale_fill_manual(values=c("#ff6c67",
                             "#a3a600",
                             "#00c377",
                             "#00b1fc",
                             "#f95dfa"))


```



```{r}
# experimentation with factors & shit to get the plot above set up right:

order <- reorder(voter_weights_blocs$Factor, sort(as.numeric(voter_weights_blocs$Factor), decreasing = TRUE))

factor(voter_weights_blocs$Factor, levels = order(unique(as.numeric(voter_weights_blocs$Factor))))
sort(as.numeric(voter_weights_blocs$Factor), decreasing = TRUE)

levels(order) <- sort(as.numeric(levels(order)))

sort(unique(as.numeric(voter_weights_blocs$Factor)))
```

## Covoting Netorks
### Covoting networks for DAO data
```{r}
# covoting network, log-scaled node sizing and coloring by Bloc

blocs <- list(which(hc_clusters == 1), 
              which(hc_clusters == 2),
              which(hc_clusters == 3),
              which(hc_clusters == 4),
              which(hc_clusters == 5))
# g <- qgraph(1 - dist_mat, groups = blocs, layout='spring', vsize=weights)

# https://cran.r-project.org/web/packages/qgraph/qgraph.pdf
g <- qgraph(1 - dist_mat, 
            layout='spring', 
            repulsion=0.8,
            # threshold = 0.1,
            # nodes:
            groups = blocs,
            vsize = 1+log(weights),
            labels = FALSE,
            borders = FALSE,
            # edges:
            edge.color = "#565656",
            colFactor = 3,
            # styling:
            # legend = TRUE,
            palette = "ggplot2",
            aspect = TRUE
            # title = "PoH Voter Similarity Network"
            )
```
```{r}
# Grey, non-node-scaled version of covoting network

g <- qgraph(1 - dist_mat, 
            layout='spring', 
            repulsion=0.8,
            # nodes:
            vsize = 1,
            color = "grey",
            labels = FALSE,
            # edges:
            edge.color = "#565656",
            colFactor = 3,
            # styling:
            palette = "ggplot2",
            aspect = TRUE
            )
```

### Covoting networks for senate data
```{r}
senate_votes <- read.csv("senate114_2.csv")
# senate_votes <- read.csv("senate101_1.csv")
sen_vot_mat <- data.matrix(senate_votes, rownames.force = NA)
sen_dist <- dist_make(sen_vot_mat, dice_metric)
sen_dist_mat <- as.matrix(sen_dist)
```

```{r}
# Senate stuff
sen_hc <- hclust(sen_dist, method = "complete")
sen_hc_clusters <- cutree(sen_hc, 3)

sen_blocs <- list(which(sen_hc_clusters == 1), 
              which(sen_hc_clusters == 2),
              which(sen_hc_clusters == 3))

g <- qgraph(1 - sen_dist_mat, 
            layout='spring', 
            repulsion=0.8,
            # nodes:
            vsize = 1,
            colors = c("red", "#041aba", "gray"),
            # color = "gray",
            groups = sen_blocs,
            labels = FALSE,
            # edges:
            edge.color = "#565656",
            colFactor = 3,
            # styling:
            palette = "ggplot2",
            aspect = TRUE
            )
```

## Analyze Bloc Power

```{r bloc1, include=FALSE}
bloc_dist <- v2 %>%
  group_by(Bloc, Voter.ID, Max.Weight) %>%
  nest() %>%
  group_by(Bloc) %>%
  summarize(Bloc.Weight = sum(Max.Weight))
```

```{r}
# Colors: 
# A: ff6c67
# B: a3a600
# C: 00c377
# D: 00b1fc
# E: f95dfa

bloc_dist %>% ggplot(aes(x = Bloc, y = Bloc.Weight, fill = factor(Bloc))) +
  geom_col() +
  ggtitle("Voting Power by Coalition") +
  xlab("Coalition") +
  ylab("Total Weight") + 
  labs(fill = "Coalition") +
  scale_fill_manual(values=c("#ff6c67",
                             "#a3a600",
                             "#00c377",
                             "#00b1fc",
                             "#f95dfa"))
```

### Gini
We can measure the "voting token wealth inequality" by Gini coefficient:

```{r gini, include=FALSE}
# compute the gini coefficient of a given distribution
gini <- function(dist) {
  
  # area between Lorenz curve and line of equality
  num <- 0
  for(x in dist) {
    for(y in dist) {
      num <- num + abs(x - y)
    }
  }
  
  # area below line of equality
  denom <- 2*(length(dist))^2*mean(dist)
  
  num/denom
}
```

```{r}
gini(bloc_dist$Bloc.Weight)
```


### Shapley-Shubik Index

```{r ssi, include=FALSE}
# compute Shapley-Shubik indices for a given distribution 
# params:
#   - dist: a distribution of voting power per voter
#   - thresh: the percentage of votes required to win, e.g. 0.5
ssi <- function(dist, thresh) {
  thresh_num <- ceiling(sum(dist)*thresh) # the number of votes required to win
  index_perm <- permn(1:length(dist)) # all possible voting orders
  ssis <- numeric(length(dist)) # a vector to store the S-S index for each voter
  
  # find the swing for each voting order
  for(order in index_perm) {
    tot_votes <- 0
    i <- 1
    while(tot_votes < thresh_num && i <= length(order)) { # identify the swing or determine that there is none
      tot_votes <- tot_votes + dist[order[i]]
      i <- i + 1
    }
    
    if(tot_votes >= thresh_num) { # increment the S-S index for the correct voter if the measure passed
      ssis[order[i - 1]] <- ssis[order[i - 1]] + 1
    }
  }
  
  ssis
}
# calculate the S-S indices as a probability dist; proportions rather than sums
ssi_prop <- function(dist, thresh) {
  ssis <- ssi(dist,thresh)
  ssis/sum(ssis)
}
bloc_dist$SSI <- ssi_prop(bloc_dist$Bloc.Weight, 0.5)
```


```{r}
bloc_dist %>% ggplot(aes(x = Bloc, y = SSI, fill = factor(Bloc))) +
  geom_col() +
  ggtitle("Shapley Shubik Index by Coalition") +
  xlab("Coalition") +
  ylab("Shapley-Shubik Index") + 
  labs(fill = "Coalition") +
  scale_fill_manual(values=c("#ff6c67",
                             "#a3a600",
                             "#00c377",
                             "#00b1fc",
                             "#f95dfa"))
```


### Export data for other use

```{r}
write.csv(vot_mat, "vot_mat.csv") # matrix of voting records
write.csv(v2, "votes_clean.csv") # filtered for recency & has e.g. max weight attribute
write.csv(dist_mat, "dist.csv")
```



# References